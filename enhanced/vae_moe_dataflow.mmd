# VAE + MoE Dataflow

```mermaid
flowchart TD
    %% Inference-time topology with training annotations
    x[Raw input x]
    enc[Encoder f_enc(x)]
    stats[(μ(x), logσ²(x))]
    subgraph Reparameterization (training)
        eps[ε ~ N(0, I)]
        sig[σ = exp(0.5·logσ²)]
        ztrain[z = μ + σ ⊙ ε]
    end
    zinfer[z = μ]:::inferenceOnly

    router[MoE Router g(z): scores → gates]
    gateSoft[Soft or Top-k gates p]

    subgraph Experts {E_i}
        dir TB
        e1[E1(z)]
        e2[E2(z)]
        eN[...]
    end

    combine[Weighted combine y_moe = Σ w_i E_i(z)]
    zprime[Latent z' (residual or replace)]
    dec[Decoder f_dec(z') → x̂]
    head[(Optional task head: ŷ)]

    %% Edges
    x --> enc --> stats
    stats -->|training| sig --> ztrain
    stats -->|inference| zinfer
    eps --> ztrain

    ztrain -. used in training .-> router
    zinfer -. used in inference .-> router

    router --> gateSoft --> e1 & e2 & eN
    e1 --> combine
    e2 --> combine
    eN --> combine
    combine --> zprime --> dec
    zprime --> head

    %% Notes
    classDef inferenceOnly fill:#eef,stroke:#99f,color:#000
```

## Training signals and objective
```mermaid
flowchart LR
    subgraph Likelihood
        x[x] --> dec2[Decoder f_dec(z')] --> xhat[x̂]
        Lrec[Reconstruction loss L_rec = -log p(x|x̂)]
        xhat --> Lrec
    end

    subgraph Posterior
        stats2[(μ, logσ²)]
        prior[(p(z)=N(0,I))]
        Lkl[KL term L_kl = D_KL(q(z|x) || p(z))]
        stats2 --> Lkl
        prior --> Lkl
    end

    subgraph MoE Path
        z2[z]
        router2[Router g(z)]
        gates[p (soft or top-k)]
        E[Experts]
        ymoe[y_moe]
        zprime2[z']
        z2 --> router2 --> gates --> E --> ymoe --> zprime2
        Laux[Aux: load-balance, entropy, importance variance]
        router2 --> Laux
    end

    subgraph Task (optional)
        zprime2 --> head2[Task head] --> yhat[ŷ]
        Ltask[Task loss]
        yhat --> Ltask
    end

    Ltot[L_total = L_rec + β·L_kl + λ_lb·L_aux + λ_task·L_task]
    Lrec --> Ltot
    Lkl --> Ltot
    Laux --> Ltot
    Ltask --> Ltot
```

## Notes
- Reparameterization (training): z = μ + σ ⊙ ε enables gradients to flow into μ, σ.
- Inference typically uses z = μ; can sample for stochastic generation.
- Gating: soft uses all experts; hard top-k selects K experts (renormalize weights).
- Combine: y_moe = Σ_i w_i E_i(z); z' = z + y_moe (residual) or z' = y_moe (replace).
- Loss routing and gradients:
  - Selected experts receive gradients; weights scale contributions. Capacity may drop tokens (no expert gradient for dropped ones).
  - Router trained via path-wise gradients and auxiliary load-balancing/entropy terms.
  - KL term updates encoder stats only; decoder/task head backprop through z' → experts → router → z → encoder (via reparameterization).
- Schedules: KL warmup (β), router temperature τ annealing, optional router-logit noise early in training.

